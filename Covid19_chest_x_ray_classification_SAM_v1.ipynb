{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Covid19 chest x-ray classification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epGd8bfZQO1l"
      },
      "source": [
        "**Import Python module**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLgNmD86woYH"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOhMJSUNdlut"
      },
      "source": [
        "#Load data to Google Colab\n",
        "local_zip = '/content/Data.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()\n",
        "DATA_PATH = \"Data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APPiNun0QqIv"
      },
      "source": [
        "**Data pre-processing(Augmentation + DataLoader)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWSAGy5SxxiB"
      },
      "source": [
        "def get_count_metrics(folder, data_path=DATA_PATH):\n",
        "\n",
        "    train_dir = os.path.join(data_path, folder)\n",
        "    list_p = os.listdir(os.path.join(train_dir,'PNEUMONIA')) # dir is your directory path\n",
        "    num_p = len(list_p)\n",
        "    list_n = os.listdir(os.path.join(train_dir,'NORMAL')) # dir is your directory path\n",
        "    num_n = len(list_n)\n",
        "    list_c = os.listdir(os.path.join(train_dir,'COVID19')) # dir is your directory path\n",
        "    num_c = len(list_c)\n",
        "    count_tuple = (int(num_n), int(num_p), int(num_c))\n",
        "    #count_tuple = (int(num_p), int(num_c))\n",
        "    #raise NotImplementedError\n",
        "\n",
        "    #return number_normal, number_pneumonia\n",
        "    return count_tuple\n",
        "\n",
        "def load_data(data_path=DATA_PATH):\n",
        "    \n",
        "    '''\n",
        "    TODO: Implement this function to return the data loader for \n",
        "    train and validation dataset. Set batchsize to 32.\n",
        "    \n",
        "    You should add the following transforms (https://pytorch.org/docs/stable/torchvision/transforms.html):\n",
        "        1. transforms.RandomResizedCrop: the images should be cropped to 224 x 224\n",
        "        2. transforms.ToTensor: just to convert data/labels to tensors\n",
        "    You should set the *shuffle* flag for *train_loader* to be True, and False for *val_loader*.\n",
        "    \n",
        "    HINT: Consider using `torchvision.datasets.ImageFolder`.\n",
        "    '''\n",
        "\n",
        "    import torchvision\n",
        "    import torchvision.datasets as datasets\n",
        "    import torchvision.transforms as transforms\n",
        "\n",
        "    # your code here\n",
        "    transform_dict = {\n",
        "        'train': transforms.Compose(\n",
        "        [transforms.Resize(224),\n",
        "         transforms.RandomResizedCrop(224),\n",
        "         transforms.RandomHorizontalFlip(p=0.5),\n",
        "         transforms.RandomRotation(degrees=(-10, 10)),\n",
        "         transforms.RandomVerticalFlip(p=0.5),\n",
        "         transforms.GaussianBlur(kernel_size= (5,5),sigma=(0.1, 2.0)),\n",
        "         transforms.ToTensor(),\n",
        "         #transforms.ColorJitter(brightness = 0.5, contrast = 0.2, saturation = 0.2),\n",
        "         #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "         #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "         ]),\n",
        "        'test': transforms.Compose(\n",
        "        [transforms.Resize(224),\n",
        "         transforms.CenterCrop(224),\n",
        "         transforms.RandomHorizontalFlip(p=0.5),\n",
        "         transforms.RandomRotation(degrees=(-10, 10)),\n",
        "         transforms.RandomVerticalFlip(p=0.5),\n",
        "         transforms.GaussianBlur(kernel_size= (5,5),sigma=(0.1, 2.0)),\n",
        "         transforms.ToTensor(),\n",
        "         #transforms.ColorJitter(brightness = 0.5, contrast = 0.2, saturation = 0.2),\n",
        "         #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "         #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "         ])}\n",
        "    \n",
        "    train_data = datasets.ImageFolder(root=data_path + '/train', transform=transform_dict['train'])\n",
        "    print(train_data.class_to_idx)\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "    #print(train_loader.class_to_idx)\n",
        "    print('train_loader - len', len(train_loader))\n",
        "    print('train_loader - type', type(train_loader))\n",
        "    \n",
        "    test_data = datasets.ImageFolder(root=data_path + '/test', transform=transform_dict['test'])\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False)\n",
        "    print('test_loader - len', len(test_loader))\n",
        "    print('test_loader - type', type(test_loader))\n",
        "    #raise NotImplementedError\n",
        "    \n",
        "    return train_loader, test_loader\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLsrVnBBymY0"
      },
      "source": [
        "assert type(get_count_metrics('train')) is tuple\n",
        "assert type(get_count_metrics('test')) is tuple\n",
        "print(get_count_metrics('train'))\n",
        "print(get_count_metrics('test'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62I6_RUjg11Z"
      },
      "source": [
        "train_loader, val_loader = load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAcobmr8Q6Za"
      },
      "source": [
        "**Explore processed image**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT8aSK9H3uSs"
      },
      "source": [
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#def imshow(img, title):\n",
        "    #npimg = img.numpy()\n",
        "    #plt.figure(figsize=(15, 7))\n",
        "    #plt.axis('off')\n",
        "    #plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    #plt.title(title)\n",
        "    #plt.show()\n",
        "\n",
        "def show_batch_images(dataloader):\n",
        "    images, labels = next(iter(dataloader))\n",
        "    grid = torchvision.utils.make_grid(images, padding=20)\n",
        "    #print(labels)\n",
        "    npgrid = grid.cpu().numpy()\n",
        "    plt.figure(figsize=(30, 15))\n",
        "    plt.imshow(np.transpose(npgrid, (1, 2, 0)), interpolation='nearest')\n",
        "    #plt.title(label=[\"COVID19\" if x==0  else (\"NORMAL\" if x == 1 else \"PNEUMONIA\") for x in labels])\n",
        "    print(labels)\n",
        "    plt.title(label=[\"COVID19\" if x==0 else \"non-COVID19\" for x in labels])\n",
        "    plt.show()\n",
        "\n",
        "    #imshow(img, title=[\"COVID19\" if x==0  else (\"NORMAL\" if x == 1 else \"PNEUMONIA\") for x in labels])\n",
        "  \n",
        "for i in range(1):\n",
        "    show_batch_images(train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yygfCMzQ6FM"
      },
      "source": [
        "**Baseline ResNet18 model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdOiuP5IcKH_"
      },
      "source": [
        "import torchvision\n",
        "from torchvision import models\n",
        "    \n",
        "num_classes = 3\n",
        "model_conv = torchvision.models.resnet18(pretrained=True)\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default   \n",
        "num_ftrs = model_conv.fc.in_features        \n",
        "model_conv.fc = nn.Linear(num_ftrs, num_classes)\n",
        " \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_conv = torch.optim.SGD(model_conv.fc.parameters(), lr= 1e-4)  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QOf4ds1mgHf"
      },
      "source": [
        "#Load pretrained model and train on multiple epochs\n",
        "n_epochs = 20\n",
        "\n",
        "def train_model(model, train_dataloader, n_epoch=n_epochs, optimizer=optimizer_conv, criterion=criterion):\n",
        "    import torch.optim as optim\n",
        "\n",
        "    model.train() # prep model for training\n",
        "    \n",
        "    _START_RUNTIME = time.time()\n",
        "    \n",
        "    for epoch in range(n_epoch):\n",
        "        print(f\"Epoch {epoch} starts\")\n",
        "        curr_epoch_loss = []\n",
        "        for data, target in train_dataloader:\n",
        "            # your code here\n",
        "            #print(data.shape)\n",
        "            #inputs, labels = data\n",
        "            \n",
        "        # forward + backward + optimize\n",
        "            \n",
        "            outputs = model(data)\n",
        "            \n",
        "            loss = criterion(outputs, target)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # zero the parameter gradients\n",
        "            optimizer.step()\n",
        "            #raise NotImplementedError\n",
        "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
        "        print(\"Total train time = {:.2f} seconds\".format(time.time() - _START_RUNTIME))    \n",
        "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLH0nhJIhRLL"
      },
      "source": [
        "# get train and val data loader\n",
        "train_loader, val_loader = load_data()\n",
        "import time\n",
        "seed = 24\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "#print(model)\n",
        "model = train_model(model_conv, train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6F-lWcnAhAt"
      },
      "source": [
        "model.eval()\n",
        "Y_pred = []\n",
        "Y_test = []\n",
        "predictions, actuals = list(), list()\n",
        "for data, target in val_loader:\n",
        "        # your code here\n",
        "    Y_pred_orig = model(data)\n",
        "        #print(len(Y_pred_tag))\n",
        "    _, Y_pred_tag = torch.max(Y_pred_orig, dim = 1)\n",
        "    Y_pred_tag = Y_pred_tag.detach().numpy()\n",
        "    Y_pred_tag = Y_pred_tag.reshape(len(Y_pred_tag), 1)\n",
        "    #Y_pred = Y_pred.round()\n",
        "    Y_test = target.numpy()\n",
        "    Y_test = Y_test.reshape(len(Y_test), 1)\n",
        "    #Y_test = np.reshape(Y_test, (-1,2))\n",
        "    predictions.append(Y_pred_tag)\n",
        "    #print(predictions)\n",
        "    actuals.append(Y_test)\n",
        "    #print(actuals)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Hg7xqxGWEVa"
      },
      "source": [
        "Y_pred = np.concatenate(predictions, axis=0)\n",
        "Y_test = np.concatenate(actuals, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZyeg3-oVunl"
      },
      "source": [
        "#accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "print(Y_pred)\n",
        "#y_pred, y_true = eval_model(model, val_loader)\n",
        "acc = accuracy_score(Y_test, Y_pred)\n",
        "print((\"Validation Accuracy of ResNet18: \" + str(acc)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ-_ISZeFpq2"
      },
      "source": [
        "**SIMPLE CNN MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUsv5nz-CX2a"
      },
      "source": [
        "class SimpleCNN_SAM(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN_SAM, self).__init__()\n",
        "        self.conv1 = torch.nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=4, stride=4)\n",
        "        self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=4, stride=4)\n",
        "        self.linear1 = nn.Linear(14 * 14 * 32, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.linear2 = nn.Linear(128, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.dropout(x)\n",
        "        pred = self.linear2(x)\n",
        "\n",
        "        return pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShK23VJ8F4h6"
      },
      "source": [
        "#Load simple CNN model and train on multiple epochs\n",
        "model_CNN = SimpleCNN_SAM()\n",
        "n_epochs = 5\n",
        "def train_CNN_model(model, train_dataloader, n_epoch=n_epochs, optimizer=optimizer_conv, criterion=criterion):\n",
        "    import torch.optim as optim\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
        "\n",
        "    model.train() # prep model for training\n",
        "    \n",
        "    _START_RUNTIME = time.time()\n",
        "    \n",
        "    for epoch in range(n_epoch):\n",
        "        print(f\"Epoch {epoch} starts\")\n",
        "        curr_epoch_loss = []\n",
        "        for data, target in train_dataloader:\n",
        "            # your code here\n",
        "            #print(data.shape)\n",
        "            #inputs, labels = data\n",
        "            \n",
        "        # forward + backward + optimize\n",
        "            \n",
        "            outputs = model(data)\n",
        "            \n",
        "            loss = criterion(outputs, target)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # zero the parameter gradients\n",
        "            optimizer.step()\n",
        "            #raise NotImplementedError\n",
        "            curr_epoch_loss.append(loss.cpu().data.numpy())\n",
        "        print(\"Total train time = {:.2f} seconds\".format(time.time() - _START_RUNTIME))    \n",
        "        print(f\"Epoch {epoch}: curr_epoch_loss={np.mean(curr_epoch_loss)}\")\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmPa3kNKFS1B"
      },
      "source": [
        "#print(model)\n",
        "model_CNN = train_CNN_model(model_CNN, train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "podC8UVHLXmw"
      },
      "source": [
        "model.eval()\n",
        "Y_pred = []\n",
        "Y_test = []\n",
        "predictions, actuals = list(), list()\n",
        "for data, target in val_loader:\n",
        "        # your code here\n",
        "    Y_pred_orig = model(data)\n",
        "        #print(len(Y_pred_tag))\n",
        "    _, Y_pred_tag = torch.max(Y_pred_orig, dim = 1)\n",
        "    Y_pred_tag = Y_pred_tag.detach().numpy()\n",
        "    Y_pred_tag = Y_pred_tag.reshape(len(Y_pred_tag), 1)\n",
        "    #Y_pred = Y_pred.round()\n",
        "    Y_test = target.numpy()\n",
        "    Y_test = Y_test.reshape(len(Y_test), 1)\n",
        "    #Y_test = np.reshape(Y_test, (-1,2))\n",
        "    predictions.append(Y_pred_tag)\n",
        "    #print(predictions)\n",
        "    actuals.append(Y_test)\n",
        "    #print(actuals)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkkQm2z7LtLt"
      },
      "source": [
        "Y_pred = np.concatenate(predictions, axis=0)\n",
        "Y_test = np.concatenate(actuals, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3DqeOYfLz-S"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(Y_pred)\n",
        "#y_pred, y_true = eval_model(model, val_loader)\n",
        "acc = accuracy_score(Y_test, Y_pred)\n",
        "print((\"Validation Accuracy of Simple CNN: \" + str(acc)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}